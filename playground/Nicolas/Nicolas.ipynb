{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Requirements\n",
    "First, we import all the packages and modules we need for our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Material\n",
    "First, we retrieve our training and test datasets from the csv files provided by Kaggle and store them in Pandas DataFrame objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (1460, 81)\n",
      "test: (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../../data/train.csv')\n",
    "test_df = pd.read_csv('../../data/test.csv')\n",
    "\n",
    "print 'train:', train_df.shape\n",
    "print 'test:', test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later use, we store the feature labels of our training data in two variables, separated by categorical and continious features. Additionally, we store the label of our target variable (SalePrice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CAT_VARS = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', \n",
    "            'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1',\n",
    "            'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n",
    "            'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond',\n",
    "            'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
    "            'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n",
    "            'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish',\n",
    "            'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n",
    "            'MoSold', 'SaleType', 'SaleCondition']\n",
    "CONT_VARS = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt',\n",
    "             'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n",
    "             'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea',\n",
    "             'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n",
    "             'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars',\n",
    "             'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n",
    "             'ScreenPorch', 'PoolArea', 'MiscVal', 'YrSold']\n",
    "\n",
    "TARGET_VAR = ['SalePrice',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "The house IDs are currently saved in the datasets as ordinary columns. Pandas, however, allows for explicitely specifying indeces, i.e. row labels (much like our feature/column labels). The following makes sure we use our own IDs as indeces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.set_index('Id', inplace=True)\n",
    "test_df.set_index('Id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains of both categorial and continuous values. The following makes sure Pandas knows about this distinction and does not confuse the two by explicetely defining categorial features as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for cat_var in CAT_VARS:\n",
    "    train_df[cat_var].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some prediction models require continuous features only. Thus, dummify() converts categorial features with *m* different possible classes into *m* new features (columns) – one feature for each class. These dummified features are either 1 or 0. \n",
    "\n",
    "For clarification of what's happening, consider the following example:\n",
    "\n",
    "| Id | Street | → | Street_Gravel | Street_Paved |\n",
    "|----|--------|---|---------------|--------------|\n",
    "| 1  | Gravel | → | 1             | 0            |\n",
    "| 2  | Paved  | → | 0             | 1            |\n",
    "| 3  |        | → | 0             | 0            |\n",
    "| 4  | Paved  | → | 0             | 1            |\n",
    "\n",
    "We apply this dummification both to our training and to our test data. A nice side effect of that is that we do not have to deal with missing categorical values – they are simply set 0 in all the dummy columns (as shown in house 3 of the above example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (1460, 314)\n",
      "test: (1459, 296)\n"
     ]
    }
   ],
   "source": [
    "def dummify(data):\n",
    "    # First, we save the data into two new DataFrames, split into categorical and\n",
    "    # continous features.\n",
    "    cont_df = data[CONT_VARS]\n",
    "    cat_df = data[CAT_VARS]\n",
    "    \n",
    "    # We iterate over each categorical variable, calculate the dummy variables,\n",
    "    # insert them into the cat_df DataFrame, and, finally, delete the original\n",
    "    # (categorical) feature from cat_df.\n",
    "    for cat_var in CAT_VARS:\n",
    "        dummies = pd.get_dummies(data[cat_var], prefix=cat_var)\n",
    "        cat_df = cat_df.join(dummies)\n",
    "        del cat_df[cat_var]\n",
    "        \n",
    "    # This merges the continuous and categorical features back into one\n",
    "    # DataFrame *result_df*.\n",
    "    result_df = cat_df.join(cont_df)\n",
    "    \n",
    "    # Up to this point, the SalePrice is missing in our newly created DataFrame\n",
    "    # result_df. Here we try to insert it again. This might fail because there\n",
    "    # actually is no target variable in our test set (only in the train set). So,\n",
    "    # if adding the (potentially missing) SalePrice fails, we just go on without\n",
    "    # adding it.\n",
    "    try:\n",
    "        result_df = result_df.join(data[TARGET_VAR])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Finally, we dummify both train_df and test_df and print their shape to see how\n",
    "# the number of columns has increased.\n",
    "train_df = dummify(train_df)\n",
    "test_df = dummify(test_df)\n",
    "\n",
    "print 'train:', train_df.shape\n",
    "print 'test:', test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our columns have changed during dummifying the data, we store the altered list of feature labels in the train data in a new list. As our target variable is not a feature we use for prediction, we exclude it from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VARS_X = list(train_df.columns)\n",
    "VARS_X.remove(TARGET_VAR[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dummified our test and train data separately. As not every class (category value) used in the train data is also used in the test data, the latter contains less new dummy columns. In order to make sure to realign the feature space (i.e. get the same number of columns), we check for columns which are present in the train data but missing in the test data. We then add these missing columns to the test data and fill them with zeros (as they apparently aren't present in the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (1460, 314)\n",
      "test: (1459, 314)\n"
     ]
    }
   ],
   "source": [
    "cols_missing_in_test = set(VARS_X) - set(test_df.columns)\n",
    "for col in cols_missing_in_test:\n",
    "    test_df[col] = 0\n",
    "\n",
    "print 'train:', train_df.shape\n",
    "print 'test:', test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**TO DO:**</font> As outliers bias our model, they need to be eliminated from the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO: ELIMINATE OUTLIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the train and the test data have a lot of missing values. The categorical variables are already covered but we still need to fill the gaps of the continuous features. The following fills all missing values by the entire column's mean. Note that the values used to fill the missing test data cells need to be based on the train data's column means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.fillna(train_df.mean())\n",
    "test_df = test_df.fillna(train_df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scales of our features vary to a great extent. Whereas the categorical features range from 0 to 1, continuous features like *BsmtUnfSF* range from 0 to well over a thousand. Some prediction models require features within the same scale. One way to achieve this is standardizing the data using z-cores. It's a convention which recalculates a column so that the mean equals 0 and one standard deviation equals 1. This way, the data is distributed more or less around 0. The scaler is learned based on the train data and subsequently applied to the train data.\n",
    "\n",
    "As this is not required for Linear Regression, the code is commented out for now. However, we might need this for other prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scaler = preprocessing.StandardScaler().fit(train_df[VARS_X])\n",
    "# train_df[VARS_X] = scaler.fit_transform(train_df[VARS_X])\n",
    "# test_df[VARS_X] = scaler.fit_transform(test_df[VARS_X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a sense of how the data looks at this point, we export our train and test data as csv files. After running the next code block, they can be found in the same folder as this notebook. Note that you might encounter errors if you try to export the files while still having an older version opened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('clean_train.csv', sept=',', index=False)\n",
    "test_df.to_csv('clean_test.csv', sept=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last not least, we split our train data into two smaller fractions: into a train and a validation set. This allows us two measure the performance of our predictions (without involving the test set which we can only evaluate by uploading it to Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (80%):\n",
      "(1168, 314)\n",
      "validation (20%):\n",
      "(292, 314)\n"
     ]
    }
   ],
   "source": [
    "# Store all the feature labels of train_df into a list; remove the SalePrice.\n",
    "features = train_df.columns.tolist()\n",
    "features.remove(TARGET_VAR[0])\n",
    "\n",
    "# Generate the training set. Set random_state to be able to replicate results.\n",
    "# Our train data will contain 80% of train_df.\n",
    "train = train_df.sample(frac=0.8, random_state=1)\n",
    "\n",
    "# Select anything not in the training set (20%) and put it in the validation set.\n",
    "validation = train_df.loc[~train_df.index.isin(train.index)]\n",
    "\n",
    "print \"train (80%):\"\n",
    "print train.shape\n",
    "print \"validation (20%):\"\n",
    "print validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Now we're finally ready two use our preprocessed data for training a linear regression model based on the 80 % train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Sqared Error: 507650749.339\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model class.\n",
    "linear_regression_model = LinearRegression()\n",
    "\n",
    "# Fit the model to the 80% training data.\n",
    "linear_regression_model.fit(train[features], train[TARGET_VAR[0]])\n",
    "\n",
    "# Generate our predictions for the validation set.\n",
    "predictions = linear_regression_model.predict(validation[features])\n",
    "\n",
    "# Compute error between our validation predictions and the actual values.\n",
    "print 'Mean Sqared Error:', mean_squared_error(predictions, validation[TARGET_VAR[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy with the mean squared error (the lower the better)? If yes, we can improve our model by using the entire 100% of our train_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression_model = LinearRegression()\n",
    "linear_regression_model.fit(train_df[features], train_df[TARGET_VAR[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this improved model to predict the housing prices of our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = linear_regression_model.predict(test_df[features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to a lack of a validation set, we are now no longer able to compute the mean squared error. But by uploading our predictions for the test set to Kaggle, we can get an even better sense of how good we're doing. This requires preparing a csv file according to Kaggle's requirements: one column with the ID, and one column with the predicted SalePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission: (1459, 2)\n"
     ]
    }
   ],
   "source": [
    "# Get the IDs from the indeces we earlier stored in test_df.\n",
    "ids = test_df.index.values\n",
    "\n",
    "# Stack the IDs and predictions into a numpy array and transpose it into\n",
    "# vertical form.\n",
    "submission = np.vstack((ids, predictions)).T\n",
    "\n",
    "# Convert the submission array into a Pandas DataFrame object.\n",
    "submission = pd.DataFrame(data=submission, columns=['Id', 'SalePrice'])\n",
    "\n",
    "# Convert Id from float to int to avoid .0 notation.\n",
    "submission['Id'] = submission['Id'].astype(int)\n",
    "\n",
    "# Print the shape of the newly created submission DataFrame.\n",
    "print 'Submission:', submission.shape\n",
    "\n",
    "# Export the submissions to a csv file.\n",
    "submission.to_csv('submission.csv', sept=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Random Forest is another prediction method we could try. The following code sort of works but is not really thought through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the model with some parameters.\n",
    "# model = RandomForestRegressor(n_estimators=100, min_samples_leaf=10, random_state=1)\n",
    "\n",
    "# Fit the model to the data.\n",
    "# model.fit(train[features], train[target])\n",
    "\n",
    "# Make predictions.\n",
    "# predictions = model.predict(validation[features])\n",
    "\n",
    "# Compute the error.\n",
    "# mean_squared_error(predictions, validation[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
