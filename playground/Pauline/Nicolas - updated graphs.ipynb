{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Requirements\n",
    "First, we import all the packages and modules we need for our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# Suppress scientific notation of float numbers in numpy arrays.\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Material\n",
    "First, we retrieve our training and test datasets from the csv files provided by Kaggle and store them in Pandas DataFrame objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/train.csv')\n",
    "test_kaggle = pd.read_csv('../../data/test.csv')\n",
    "\n",
    "print 'train:', train.shape\n",
    "print 'test:', test_kaggle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'train:', train.shape\n",
    "print 'test:', test_kaggle.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later use, we store the feature labels of our training data in two variables, separated by categorical and continious features. Additionally, we store the label of our target variable (SalePrice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CAT_VARS = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape',\n",
    "            'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n",
    "            'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
    "            'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n",
    "            'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond',\n",
    "            'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n",
    "            'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC',\n",
    "            'CentralAir', 'Electrical', 'KitchenQual', 'Functional',\n",
    "            'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n",
    "            'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n",
    "            'MoSold', 'SaleType', 'SaleCondition']\n",
    "CONT_VARS = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n",
    "             'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n",
    "             'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF',\n",
    "             'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
    "             'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n",
    "             'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
    "             'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch',\n",
    "             'PoolArea', 'MiscVal', 'YrSold', 'LiveabilityScale', 'Income2010', 'MasterDegree']\n",
    "\n",
    "TARGET_VAR = ['SalePrice', ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "The house IDs are currently saved in the datasets as ordinary columns. Pandas, however, allows for explicitely specifying indeces, i.e. row labels (much like our feature/column labels). The following makes sure we use our own IDs as indeces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.set_index('Id', inplace=True)\n",
    "test_kaggle.set_index('Id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains of both categorial and continuous values. The following makes sure Pandas knows about this distinction and does not confuse the two by explicetely defining categorial features as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cat_var in CAT_VARS:\n",
    "    train[cat_var].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some prediction models require continuous features only. Thus, dummify() converts categorial features with *m* different possible classes into *m* new features (columns) – one feature for each class. These dummified features are either 1 or 0. \n",
    "\n",
    "For clarification of what's happening, consider the following example:\n",
    "\n",
    "| Id | Street | → | Street_Gravel | Street_Paved |\n",
    "|----|--------|---|---------------|--------------|\n",
    "| 1  | Gravel | → | 1             | 0            |\n",
    "| 2  | Paved  | → | 0             | 1            |\n",
    "| 3  |        | → | 0             | 0            |\n",
    "| 4  | Paved  | → | 0             | 1            |\n",
    "\n",
    "We apply this dummification both to our training and to our test data. A nice side effect of that is that we do not have to deal with missing categorical values – they are simply set 0 in all the dummy columns (as shown in house 3 of the above example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dummify(data, update_cat_vars=False):\n",
    "    # This allows us to alter the global variable CAT_VARS within the function.\n",
    "    global CAT_VARS\n",
    "\n",
    "    # First, we save the data into two new DataFrames, split into categorical\n",
    "    # and continous features.\n",
    "    cont_df = data[CONT_VARS]\n",
    "    cat_df = data[CAT_VARS]\n",
    "    cat_vars_new = list(CAT_VARS)\n",
    "\n",
    "    # We iterate over each categorical variable, calculate the dummy variables,\n",
    "    # insert them into the cat_df DataFrame, and, finally, delete the original\n",
    "    # (categorical) feature from cat_df.\n",
    "    # Additionally, we save the labels of our newly created dummy features in\n",
    "    # CAT_VARS_new.\n",
    "    for cat_var in CAT_VARS:\n",
    "        dummies = pd.get_dummies(data[cat_var], prefix=cat_var)\n",
    "        cat_df = cat_df.join(dummies)\n",
    "        del cat_df[cat_var]\n",
    "\n",
    "        cat_vars_new.remove(cat_var)\n",
    "        cat_vars_new = cat_vars_new + dummies.columns.values.tolist()\n",
    "\n",
    "    # This merges the continuous and categorical features back into one\n",
    "    # DataFrame *result_df*.\n",
    "    result_df = cat_df.join(cont_df)\n",
    "\n",
    "    # Up to this point, the SalePrice is missing in our newly created DataFrame\n",
    "    # result_df. Here we try to insert it again. This might fail because there\n",
    "    # actually is no target variable in our test set (only in the train set).\n",
    "    # So, if adding the (potentially missing) SalePrice fails, we just go on\n",
    "    # without adding it.\n",
    "    try:\n",
    "        result_df = result_df.join(data[TARGET_VAR])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Only update the global CAT_VARS labels if we passed the argument\n",
    "    # update_cat_vars to the function.\n",
    "    if (update_cat_vars):\n",
    "        CAT_VARS = cat_vars_new\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Finally, we dummify both train_df and test_df and print their shape to see\n",
    "# how the number of columns has increased. When running the function with the\n",
    "# train data we tell it to update our newly created dummy feature labels in\n",
    "# the CAT_VARS variable.\n",
    "test_kaggle = dummify(test_kaggle)\n",
    "train = dummify(train, update_cat_vars=True)\n",
    "\n",
    "print 'train:', train.shape\n",
    "print 'test:', test_kaggle.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our columns have changed during dummifying the data, we store the altered list of feature labels in the train data in a new list. As our target variable is not a feature we use for prediction, we exclude it from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VARS_X = list(train.columns)\n",
    "VARS_X.remove(TARGET_VAR[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dummified our test and train data separately. As not every class (category value) used in the train data is also used in the test data, and vice versa, our columns might no longer match. In order to make sure to realign the number of features (i.e. get the same number of columns), we check for columns which are present in the train data but missing in the test data, add these missing columns to the test data and fill them with zeros (as they apparently aren't present in the test set). Columns present in the test set but not in the train set are entirely dropped from the test set as our model cannot use them for prediction anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (1460, 311)\n",
      "test: (1459, 310)\n"
     ]
    }
   ],
   "source": [
    "cols_missing_in_test = set(VARS_X) - set(test_kaggle.columns)\n",
    "for col in cols_missing_in_test:\n",
    "    test_kaggle[col] = 0\n",
    "    \n",
    "cols_missing_in_train = set(test_kaggle.columns) - set(VARS_X)\n",
    "for col in cols_missing_in_train:\n",
    "    del test_kaggle[col]\n",
    "    CAT_VARS = list(set(CAT_VARS) - set(cols_missing_in_train))\n",
    "\n",
    "print 'train:', train.shape\n",
    "print 'test:', test_kaggle.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**TO DO:**</font> As outliers potentially bias our model (depending on the prediction model), they might need to be eliminated from the train data. In the following example we simply drop all the lines that differ in at least one column by more than 3 standard deviations from the column mean. But this seems to be a bad idea as it just makes our dataset a lot smaller. We get better mean squared errors when slicing our train data into a subset + validation set. But as soon as we apply it to the test data, Kaggle reports lower scores – so we are sort of punished for training or model based on a smaller train dataset. So we might need to investigate more sophisticated methods of outlier handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop each row which has an outlier in at least one cell. An outlier is\n",
    "# defined as being more than 3 standard devidations above or below the\n",
    "# column mean.\n",
    "\n",
    "# WARNING: This simple approach does not really work well as it drops nearly\n",
    "#          all instances. A quick research revealed that we should either use\n",
    "#          a more sophisticated outlier detection model (e.g. k-nearest\n",
    "#          neighbor clustering), or that we should use a prediction model more\n",
    "#          robust to outliers (e.g. Random Forest).\n",
    "\n",
    "# for cont_var in CONT_VARS:\n",
    "#    train = train[np.abs(train[cont_var] - train[cont_var].mean()) <= (3 * train[cont_var].std())]\n",
    "\n",
    "# train = eliminate_outliers(train)\n",
    "# print 'train:', train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the train and the test data have a lot of missing values. The categorical variables are already covered but we still need to fill the gaps of the continuous features. The following fills all missing values by the entire column's mean. Note that the values used to fill the missing test data cells need to be based on the train data's column means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.fillna(train.mean())\n",
    "test_kaggle = test_kaggle.fillna(train.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scales of our features vary to a great extent. Whereas the categorical features range from 0 to 1, continuous features like *BsmtUnfSF* range from 0 to well over a thousand. Some prediction models require features within the same scale. One way to achieve this is standardizing the data using z-cores. It's a convention which recalculates a column so that the mean equals 0 and one standard deviation equals 1. This way, the data is distributed more or less around 0. The scaler is learned based on the train data and subsequently applied to the train data. As our binary category variables already are within the desired scale, we only apply the standardization to the continous variables (as suggested by <a href=\"http://andrewgelman.com/2009/07/11/when_to_standar/\" target=\"_blank\">Gelman, 2009</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WARNING: The following code might not work as expected. It created some\n",
    "#          weird negative results with the Linear Regression model. So better\n",
    "#          use it with caution.\n",
    "\n",
    "#scaler = preprocessing.StandardScaler().fit(train[CONT_VARS])\n",
    "#train[CONT_VARS] = scaler.transform(train[CONT_VARS])\n",
    "#test_kaggle[CONT_VARS] = scaler.transform(test_kaggle[CONT_VARS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a sense of how the data looks at this point, we export our train and test data as csv files. After running the next code block, they can be found in the same folder as this notebook. Note that you might encounter errors if you try to export the files while still having an older version opened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.to_csv('clean_train.csv', sept=',', index=False)\n",
    "test_kaggle.to_csv('clean_test.csv', sept=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last not least, we split our train data into two smaller fractions: into a train and a validation set. This allows us two measure the performance of our predictions (without involving the test set which we can only evaluate by uploading it to Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (80%):\n",
      "(1168, 311)\n",
      "validation (20%):\n",
      "(292, 311)\n"
     ]
    }
   ],
   "source": [
    "# Store all the feature labels of train_df into a list; remove the SalePrice.\n",
    "features = train.columns.tolist()\n",
    "features.remove(TARGET_VAR[0])\n",
    "\n",
    "# Generate the training set. Set random_state to be able to replicate results.\n",
    "# Our train data will contain 80% of train_df.\n",
    "train_frac = train.sample(frac=0.8, random_state=1)\n",
    "\n",
    "# Select anything not in the training set (20%) and put it in the validation\n",
    "# set.\n",
    "validation = train.loc[~train.index.isin(train_frac.index)]\n",
    "\n",
    "print \"train (80%):\"\n",
    "print train_frac.shape\n",
    "print \"validation (20%):\"\n",
    "print validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Now we're finally ready two use our preprocessed data for training a linear regression model based on the 80 % train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Sqared Error: 578237455.167\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model class.\n",
    "linear_regression_model = LinearRegression() #(normalize=True)?\n",
    "\n",
    "# Fit the model to the 80% training data.\n",
    "linear_regression_model.fit(train_frac[features], train_frac[TARGET_VAR[0]])\n",
    "\n",
    "# Generate our predictions for the validation set.\n",
    "predictions = linear_regression_model.predict(validation[features])\n",
    "\n",
    "# Compute error between our validation predictions and the actual values.\n",
    "print 'Mean Sqared Error:', mean_squared_error(predictions,\n",
    "                                               validation[TARGET_VAR[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we print a learning curve to examine if our model suffers from overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def learning_curve(X_train, y_train, X_test, y_test, model, scale):\n",
    "def learning_curve(X_train, y_train, X_test, y_test, model):\n",
    "    # We will vary the training set size so that we have 50 different sizes\n",
    "    sizes = np.round(np.linspace(1, len(X_train), 20))\n",
    "    train_err = np.zeros(len(sizes))\n",
    "    test_err = np.zeros(len(sizes))\n",
    "    #X_train = X_train.sample(frac=1) # random_state=42\n",
    "    for i, s in enumerate(sizes):\n",
    "        # Create and fit the regressor model\n",
    "        model.fit(X_train[:int(s)], y_train[:int(s)])\n",
    "        model.fit(X_test[:int(s)], y_test[:int(s)])\n",
    "        # Find the performance on the training and testing set\n",
    "        train_err[i] = mean_squared_error(y_train[:int(s)], model.predict(X_train[:int(s)]))\n",
    "        test_err[i] = mean_squared_error(y_test, model.predict(X_test))\n",
    "        \n",
    "    # Plot learning curve graph\n",
    "    learning_curve_graph(sizes, train_err, test_err)\n",
    "\n",
    "def learning_curve_graph(sizes, train_err, test_err):\n",
    "    plt.figure()\n",
    "    plt.title('Learning Curve')\n",
    "    plt.plot(sizes, test_err, lw=2, label='test error')\n",
    "    plt.plot(sizes, train_err, lw=2, label='training error')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Training Size')\n",
    "    plt.ylabel('Error')\n",
    "    #plt.ylim(scale)\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_curve(train_frac[features],\n",
    "               train_frac[TARGET_VAR[0]],\n",
    "               validation[features],\n",
    "               validation[TARGET_VAR[0]],\n",
    "               LinearRegression())\n",
    "\n",
    "#learning_curve(train_frac[features],\n",
    "               #train_frac[TARGET_VAR[0]],\n",
    "               #validation[features],\n",
    "               #validation[TARGET_VAR[0]],\n",
    "               #LinearRegression(), 1000000000, 20000000000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy with the mean squared error (the lower the better)? If yes, we can improve our model by using the entire 100% of our train_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression_model = LinearRegression() #(normalize=True)?\n",
    "linear_regression_model.fit(train[features], train[TARGET_VAR[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this improved model to predict the housing prices of our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459L,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = linear_regression_model.predict(test_kaggle[features])\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to a lack of a validation set, we are now no longer able to compute the mean squared error. But by uploading our predictions for the test set to Kaggle, we can get an even better sense of how good we're doing. This requires preparing a csv file according to Kaggle's requirements: one column with the ID, and one column with the predicted SalePrice. We create a function we can also reuse for other models than Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def export_csv(predictions, model):\n",
    "    # Get the IDs from the indeces we earlier stored in test_df.\n",
    "    ids = test_kaggle.index.values\n",
    "\n",
    "    # Stack the IDs and predictions into a numpy array and transpose it into\n",
    "    # vertical form.\n",
    "    submission = np.vstack((ids, predictions)).T\n",
    "\n",
    "    # Convert the submission array into a Pandas DataFrame object.\n",
    "    submission = pd.DataFrame(data=submission, columns=['Id', 'SalePrice'])\n",
    "\n",
    "    # Convert Id from float to int to avoid .0 notation.\n",
    "    submission['Id'] = submission['Id'].astype(int)\n",
    "    \n",
    "    # Convert possible negative predicted values to 0\n",
    "    submission[submission < 0] = 0\n",
    "\n",
    "    # Print the shape of the newly created submission DataFrame.\n",
    "    print 'Submission:', submission.shape\n",
    "    \n",
    "    # Export the submissions to a csv file.\n",
    "    submission.to_csv('submission_' + model + '.csv', sept=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the export_csv function to print our Linear Regression predictions to a csv file called submission_linear_regression.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission: (1459, 2)\n"
     ]
    }
   ],
   "source": [
    "export_csv(predictions, 'linear_regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "Ridge regression is a linear regression model with regularization. The alpha value which determines the degree to which high weights are penalized in order to simplify the function and to avoid overfitting needs to be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a list of alpha values to be evaluated.\n",
    "alphas = [0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10,\n",
    "          11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 30, 40, 50, 60, 70, 80,\n",
    "          90, 100]\n",
    "\n",
    "# Create empty array that we will later fill with our alpha values and\n",
    "# their corresponding squared errors on the validation set.\n",
    "mean_squared_errors = np.zeros((len(alphas), 2))\n",
    "\n",
    "# Itereate over the list of alpha values.\n",
    "for i, alph in enumerate(alphas):\n",
    "    # Initialize a new Ridge regression model with the alpha value.\n",
    "    ridge_regression_model = Ridge(alpha=alph)\n",
    "    \n",
    "    # Train the model on the 80% train data set.\n",
    "    ridge_regression_model.fit(train_frac[features],\n",
    "                               train_frac[TARGET_VAR[0]])\n",
    "    \n",
    "    # Predict the values for the validation set.\n",
    "    predictions = ridge_regression_model.predict(validation[features])\n",
    "    \n",
    "    # Compute the mean squared error.\n",
    "    error = mean_squared_error(predictions, validation[TARGET_VAR[0]])\n",
    "    \n",
    "    # Save the alpha value and its resulting mean squared error into\n",
    "    # the array we created before.\n",
    "    mean_squared_errors[i] = [alph, error]\n",
    "\n",
    "# Retrieve and print the alpha value which resulted in the lowest mean\n",
    "# squared error.\n",
    "alph = mean_squared_errors[np.argmin(mean_squared_errors, axis=0)[1]][0]\n",
    "print 'Optimal Alpha: %s' % alph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our model with the optimized alpha value, we print a learning curve which indicates if our model suffers from overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_curve(train_frac[features],\n",
    "               train_frac[TARGET_VAR[0]],\n",
    "               validation[features],\n",
    "               validation[TARGET_VAR[0]],\n",
    "               Ridge(alpha=alph))\n",
    "\n",
    "#learning_curve(train_frac[features],\n",
    "               #train_frac[TARGET_VAR[0]],\n",
    "               #validation[features],\n",
    "               #validation[TARGET_VAR[0]],\n",
    "               #Ridge(alpha=alph), [0, 10000000000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything looks good, we can train the model on the full train dataset and save our predictions to a csv file for submission to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission: (1459, 2)\n"
     ]
    }
   ],
   "source": [
    "# Train a new Ridge regression model on the 100% train data using the\n",
    "# optimized alpha value and predict the housing prices for the Kaggle\n",
    "# test set.\n",
    "ridge_regression_model = Ridge(alpha=alph)\n",
    "ridge_regression_model.fit(train[features], train[TARGET_VAR[0]])\n",
    "predictions = ridge_regression_model.predict(test_kaggle[features])\n",
    "\n",
    "# Export the predictions to a csv file for submission.\n",
    "export_csv(predictions, 'ridge_regression')\n",
    "\n",
    "# not really good result on Kaggel : 0.41435"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor\n",
    "Now we can try out another method: Random Forest Regressor. It works pretty much the same like the Linear Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize hyperparameters.\n",
    "n_estimators = 100\n",
    "max_features = 'auto'\n",
    "max_depth = None\n",
    "min_samples_leaf = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features: 0.4\n"
     ]
    }
   ],
   "source": [
    "# Optimize max_features.\n",
    "\n",
    "max_features_options = [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]\n",
    "mean_squared_errors = np.zeros((len(max_features_options), 2))\n",
    "\n",
    "for i, max_features in enumerate(max_features_options):\n",
    "    random_forest_model = RandomForestRegressor(\n",
    "        random_state=42,\n",
    "        n_estimators=n_estimators,\n",
    "        max_features=max_features,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_leaf=min_samples_leaf\n",
    "    )\n",
    "    \n",
    "    random_forest_model.fit(train_frac[features],\n",
    "                            train_frac[TARGET_VAR[0]])\n",
    "    \n",
    "    predictions = random_forest_model.predict(validation[features])\n",
    "    error = mean_squared_error(predictions, validation[TARGET_VAR[0]])\n",
    "    mean_squared_errors[i] = [max_features, error]\n",
    "\n",
    "max_features = mean_squared_errors[np.argmin(mean_squared_errors, axis=0)[1]][0]\n",
    "print 'max_features: %s' % max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth: 25.0\n"
     ]
    }
   ],
   "source": [
    "# Optimize max_depth.\n",
    "\n",
    "max_depth_options = [2, 4, 6, 8, 10, 12, 15, 20, 25, 30, 50]\n",
    "mean_squared_errors = np.zeros((len(max_depth_options), 2))\n",
    "\n",
    "for i, max_depth in enumerate(max_depth_options):\n",
    "    random_forest_model = RandomForestRegressor(\n",
    "        random_state=42,\n",
    "        n_estimators=n_estimators,\n",
    "        max_features=max_features,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_leaf=min_samples_leaf\n",
    "    )\n",
    "    \n",
    "    random_forest_model.fit(train_frac[features],\n",
    "                            train_frac[TARGET_VAR[0]])\n",
    "    \n",
    "    predictions = random_forest_model.predict(validation[features])\n",
    "    error = mean_squared_error(predictions, validation[TARGET_VAR[0]])\n",
    "    mean_squared_errors[i] = [max_depth, error]\n",
    "\n",
    "max_depth = mean_squared_errors[np.argmin(mean_squared_errors, axis=0)[1]][0]\n",
    "print 'max_depth: %s' % max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_samples_leaf: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Optimize min_samples_leaf.\n",
    "\n",
    "min_samples_leaf_options = [1, 2, 4, 6, 8, 10, 12, 15, 20, 25, 30, 50]\n",
    "mean_squared_errors = np.zeros((len(min_samples_leaf_options), 2))\n",
    "\n",
    "for i, min_samples_leaf in enumerate(min_samples_leaf_options):\n",
    "    random_forest_model = RandomForestRegressor(\n",
    "        random_state=42,\n",
    "        n_estimators=n_estimators,\n",
    "        max_features=max_features,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_leaf=min_samples_leaf\n",
    "    )\n",
    "    \n",
    "    random_forest_model.fit(train_frac[features],\n",
    "                            train_frac[TARGET_VAR[0]])\n",
    "    \n",
    "    predictions = random_forest_model.predict(validation[features])\n",
    "    error = mean_squared_error(predictions, validation[TARGET_VAR[0]])\n",
    "    mean_squared_errors[i] = [min_samples_leaf, error]\n",
    "\n",
    "min_samples_leaf = mean_squared_errors[np.argmin(mean_squared_errors, axis=0)[1]][0]\n",
    "print 'min_samples_leaf: %s' % min_samples_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating learning curve for Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning_curve_forest(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Calculate the performance of the model after a set of training data.\"\"\"\n",
    "    \n",
    "    # We will vary the training set size so that we have 50 different sizes\n",
    "    sizes = np.round(np.linspace(1, len(X_train), 50))\n",
    "    train_err = np.zeros(len(sizes))\n",
    "    test_err = np.zeros(len(sizes))\n",
    " \n",
    "    for i, s in enumerate(sizes):\n",
    "        # Create and fit the regressor model\n",
    "        random_forest_model.fit(X_train[:int(s)], y_train[:int(s)])\n",
    "        random_forest_model.fit(X_test[:int(s)], y_test[:int(s)])\n",
    "        \n",
    "        # Find the performance on the training and testing set\n",
    "        train_err[i] = mean_squared_error(y_train[:int(s)], random_forest_model.predict(X_train[:int(s)]))\n",
    "        test_err[i] = mean_squared_error(y_test, random_forest_model.predict(X_test))\n",
    "        \n",
    "    # Plot learning curve graph\n",
    "    learning_curve_graph(sizes, train_err, test_err)\n",
    "\n",
    "def learning_curve_graph(sizes, train_err, test_err):\n",
    "    \"\"\"Plot training and test error as a function of the training size.\"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "    #plt.title('Random Forest: Performance vs Training Size')\n",
    "    plt.plot(sizes, test_err, lw=2, label = 'test error')\n",
    "    plt.plot(sizes, train_err, lw=2, label = 'training error')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Training Size')\n",
    "    plt.ylabel('Error')\n",
    "    plt.show()\n",
    "\n",
    "learning_curve_forest(train_frac[features], train_frac[TARGET_VAR[0]], validation[features], validation[TARGET_VAR[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#random_forest_model = RandomForestRegressor(\n",
    "    #random_state=42,\n",
    "    #n_estimators=n_estimators,\n",
    "    #max_features=max_features,\n",
    "    #max_depth=max_depth,\n",
    "    #min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "#learning_curve(train_frac[features],\n",
    "               #train_frac[TARGET_VAR[0]],\n",
    "               #validation[features],\n",
    "               #validation[TARGET_VAR[0]],\n",
    "               #random_forest_model, [0, 10000000000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, when we are happy with the Mean Squared Error and the learning curve, we can fit the model to our 100% train_df dataset, predict the values for Kaggle's test dataset, and export the results into a file called submission_random_forest_regressor.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_forest_model = RandomForestRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    max_features=max_features,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_leaf=min_samples_leaf\n",
    ")\n",
    "\n",
    "random_forest_model.fit(train[features], train[TARGET_VAR[0]])\n",
    "\n",
    "predictions = random_forest_model.predict(test_kaggle[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission: (1459, 2)\n"
     ]
    }
   ],
   "source": [
    "export_csv(predictions, 'random_forest_regressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Sqared Error: 798488237.091\n",
      "help\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "\n",
    "random_forest_model = RandomForestRegressor(max_depth=4)\n",
    "adaboost_model = AdaBoostRegressor(random_forest_model,\n",
    "                                   n_estimators=300,\n",
    "                                   random_state=rng)\n",
    "\n",
    "adaboost_model.fit(train_frac[features], train_frac[TARGET_VAR[0]])\n",
    "predictions = adaboost_model.predict(validation[features])\n",
    "\n",
    "# Compute error between our validation predictions and the actual values.\n",
    "print 'Mean Squared Error:', mean_squared_error(predictions,\n",
    "                                               validation[TARGET_VAR[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission: (1459, 2)\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Optimize parameters (see comments in the codeblock above).\n",
    "adaboost_model = AdaBoostRegressor(random_forest_model,\n",
    "                                   n_estimators=300,\n",
    "                                   random_state=rng)\n",
    "\n",
    "adaboost_model.fit(train[features], train[TARGET_VAR[0]])\n",
    "\n",
    "predictions = adaboost_model.predict(test_kaggle[features])\n",
    "\n",
    "export_csv(predictions, 'adaboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating learning curve for Adaboost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_forest_model = RandomForestRegressor(max_depth=4)\n",
    "adaboost_model = AdaBoostRegressor(random_forest_model,\n",
    "                                   n_estimators=300,\n",
    "                                   random_state=rng)\n",
    "\n",
    "learning_curve(train_frac[features],\n",
    "               train_frac[TARGET_VAR[0]],\n",
    "               validation[features],\n",
    "               validation[TARGET_VAR[0]],\n",
    "               adaboost_model)\n",
    "\n",
    "#learning_curve(train_frac[features],\n",
    "               #train_frac[TARGET_VAR[0]],\n",
    "               #validation[features],\n",
    "               #validation[TARGET_VAR[0]],\n",
    "               #adaboost_model,    [0, 10000000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
