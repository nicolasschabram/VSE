{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Requirements\n",
    "First, we import all the packages and modules we need for our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Material\n",
    "First, we retrieve our training and test datasets from the csv files provided by Kaggle and store them in Pandas DataFrame objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (1460, 314)\n",
      "test: (1459, 313)\n"
     ]
    }
   ],
   "source": [
    "print 'train:', train_df.shape\n",
    "print 'test:', test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (1460, 81)\n",
      "test: (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../../data/train.csv')\n",
    "test_df = pd.read_csv('../../data/test.csv')\n",
    "\n",
    "print 'train:', train_df.shape\n",
    "print 'test:', test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later use, we store the feature labels of our training data in two variables, separated by categorical and continious features. Additionally, we store the label of our target variable (SalePrice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CAT_VARS = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', \n",
    "            'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1',\n",
    "            'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n",
    "            'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond',\n",
    "            'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
    "            'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n",
    "            'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish',\n",
    "            'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n",
    "            'MoSold', 'SaleType', 'SaleCondition']\n",
    "CONT_VARS = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt',\n",
    "             'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n",
    "             'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea',\n",
    "             'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n",
    "             'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars',\n",
    "             'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n",
    "             'ScreenPorch', 'PoolArea', 'MiscVal', 'YrSold']\n",
    "\n",
    "TARGET_VAR = ['SalePrice',]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "The house IDs are currently saved in the datasets as ordinary columns. Pandas, however, allows for explicitely specifying indeces, i.e. row labels (much like our feature/column labels). The following makes sure we use our own IDs as indeces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.set_index('Id', inplace=True)\n",
    "test_df.set_index('Id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains of both categorial and continuous values. The following makes sure Pandas knows about this distinction and does not confuse the two by explicetely defining categorial features as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cat_var in CAT_VARS:\n",
    "    train_df[cat_var].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some prediction models require continuous features only. Thus, dummify() converts categorial features with *m* different possible classes into *m* new features (columns) – one feature for each class. These dummified features are either 1 or 0. \n",
    "\n",
    "For clarification of what's happening, consider the following example:\n",
    "\n",
    "| Id | Street | → | Street_Gravel | Street_Paved |\n",
    "|----|--------|---|---------------|--------------|\n",
    "| 1  | Gravel | → | 1             | 0            |\n",
    "| 2  | Paved  | → | 0             | 1            |\n",
    "| 3  |        | → | 0             | 0            |\n",
    "| 4  | Paved  | → | 0             | 1            |\n",
    "\n",
    "We apply this dummification both to our training and to our test data. A nice side effect of that is that we do not have to deal with missing categorical values – they are simply set 0 in all the dummy columns (as shown in house 3 of the above example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (1460, 314)\n",
      "test: (1459, 296)\n"
     ]
    }
   ],
   "source": [
    "def dummify(data, update_cat_vars=False):\n",
    "    # This allows us to alter the global variable CAT_VARS within the function.\n",
    "    global CAT_VARS\n",
    "    \n",
    "    # First, we save the data into two new DataFrames, split into categorical and\n",
    "    # continous features.\n",
    "    cont_df = data[CONT_VARS]\n",
    "    cat_df = data[CAT_VARS]\n",
    "    cat_vars_new = list(CAT_VARS)\n",
    "    \n",
    "    # We iterate over each categorical variable, calculate the dummy variables,\n",
    "    # insert them into the cat_df DataFrame, and, finally, delete the original\n",
    "    # (categorical) feature from cat_df.\n",
    "    # Additionally, we save the labels of our newly created dummy features in\n",
    "    # CAT_VARS_new.\n",
    "    for cat_var in CAT_VARS:\n",
    "        dummies = pd.get_dummies(data[cat_var], prefix=cat_var)\n",
    "        cat_df = cat_df.join(dummies)\n",
    "        del cat_df[cat_var]\n",
    "        \n",
    "        cat_vars_new.remove(cat_var)\n",
    "        cat_vars_new = cat_vars_new + dummies.columns.values.tolist()\n",
    "        \n",
    "    # This merges the continuous and categorical features back into one\n",
    "    # DataFrame *result_df*.\n",
    "    result_df = cat_df.join(cont_df)\n",
    "    \n",
    "    # Up to this point, the SalePrice is missing in our newly created DataFrame\n",
    "    # result_df. Here we try to insert it again. This might fail because there\n",
    "    # actually is no target variable in our test set (only in the train set). So,\n",
    "    # if adding the (potentially missing) SalePrice fails, we just go on without\n",
    "    # adding it.\n",
    "    try:\n",
    "        result_df = result_df.join(data[TARGET_VAR])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Only update the global CAT_VARS labels if we passed the argument\n",
    "    # update_cat_vars to the function.\n",
    "    if (update_cat_vars):\n",
    "        CAT_VARS = cat_vars_new\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Finally, we dummify both train_df and test_df and print their shape to see how\n",
    "# the number of columns has increased. When running the function with the train\n",
    "# data we tell it to update our newly created dummy feature labels in the CAT_VARS\n",
    "# variable.\n",
    "test_df = dummify(test_df)\n",
    "train_df = dummify(train_df, update_cat_vars=True)\n",
    "\n",
    "print 'train:', train_df.shape\n",
    "print 'test:', test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our columns have changed during dummifying the data, we store the altered list of feature labels in the train data in a new list. As our target variable is not a feature we use for prediction, we exclude it from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VARS_X = list(train_df.columns)\n",
    "VARS_X.remove(TARGET_VAR[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dummified our test and train data separately. As not every class (category value) used in the train data is also used in the test data, and vice versa, our columns might no longer match. In order to make sure to realign the number of features (i.e. get the same number of columns), we check for columns which are present in the train data but missing in the test data, add these missing columns to the test data and fill them with zeros (as they apparently aren't present in the test set). Columns present in the test set but not in the train set are entirely dropped from the test set as our model cannot use them for prediction anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (1460, 314)\n",
      "test: (1459, 313)\n"
     ]
    }
   ],
   "source": [
    "cols_missing_in_test = set(VARS_X) - set(test_df.columns)\n",
    "for col in cols_missing_in_test:\n",
    "    test_df[col] = 0\n",
    "    \n",
    "cols_missing_in_train = set(test_df.columns) - set(VARS_X)\n",
    "for col in cols_missing_in_train:\n",
    "    del test_df[col]\n",
    "    CAT_VARS = list(set(CAT_VARS) - set(cols_missing_in_train))\n",
    "\n",
    "print 'train:', train_df.shape\n",
    "print 'test:', test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**TO DO:**</font> As outliers potentially bias our model (depending on the prediction model), they might need to be eliminated from the train data. In the following example we simply drop all the lines that differ in at least one column by more than 3 standard deviations from the column mean. But this seems to be a bad idea as it just makes our dataset a lot smaller. We get better mean squared errors when slicing our train data into a subset + validation set. But as soon as we apply it to the test data, Kaggle reports lower scores – so we are sort of punished for training or model based on a smaller train dataset. So we might need to investigate more sophisticated methods of outlier handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop each row which has an outlier in at least one cell. An outlier is\n",
    "# defined as being more than 3 standard devidations above or below the\n",
    "# column mean.\n",
    "\n",
    "# WARNING: This simple approach does not really work well as it drops nearly\n",
    "#          all instances. A quick research revealed that we should either use\n",
    "#          a more sophisticated outlier detection model (e.g. k-nearest neighbor\n",
    "#          clustering), or that we should use a prediction model more robust to\n",
    "#          outliers (e.g. Random Forest).\n",
    "\n",
    "\n",
    "#for cont_var in CONT_VARS:\n",
    "#    train_df = train_df[np.abs(train_df[cont_var] - train_df[cont_var].mean()) <= (3 * train_df[cont_var].std())]\n",
    "\n",
    "#train_df = eliminate_outliers(train_df)\n",
    "#print 'train:', train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the train and the test data have a lot of missing values. The categorical variables are already covered but we still need to fill the gaps of the continuous features. The following fills all missing values by the entire column's mean. Note that the values used to fill the missing test data cells need to be based on the train data's column means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.fillna(train_df.mean())\n",
    "test_df = test_df.fillna(train_df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scales of our features vary to a great extent. Whereas the categorical features range from 0 to 1, continuous features like *BsmtUnfSF* range from 0 to well over a thousand. Some prediction models require features within the same scale. One way to achieve this is standardizing the data using z-cores. It's a convention which recalculates a column so that the mean equals 0 and one standard deviation equals 1. This way, the data is distributed more or less around 0. The scaler is learned based on the train data and subsequently applied to the train data. As our binary category variables already are within the desired scale, we only apply the standardization to the continous variables (as suggested by <a href=\"http://andrewgelman.com/2009/07/11/when_to_standar/\" target=\"_blank\">Gelman, 2009</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WARNING: The following code might not work as expected. It created some\n",
    "#          weird negative results with the Linear Regression model. So better\n",
    "#          use it with caution.\n",
    "\n",
    "#scaler = preprocessing.StandardScaler().fit(train_df[CONT_VARS])\n",
    "#train_df[CONT_VARS] = scaler.transform(train_df[CONT_VARS])\n",
    "#test_df[CONT_VARS] = scaler.transform(test_df[CONT_VARS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a sense of how the data looks at this point, we export our train and test data as csv files. After running the next code block, they can be found in the same folder as this notebook. Note that you might encounter errors if you try to export the files while still having an older version opened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('clean_train.csv', sept=',', index=False)\n",
    "test_df.to_csv('clean_test.csv', sept=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last not least, we split our train data into two smaller fractions: into a train and a validation set. This allows us two measure the performance of our predictions (without involving the test set which we can only evaluate by uploading it to Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (80%):\n",
      "(1168, 314)\n",
      "validation (20%):\n",
      "(292, 314)\n"
     ]
    }
   ],
   "source": [
    "# Store all the feature labels of train_df into a list; remove the SalePrice.\n",
    "features = train_df.columns.tolist()\n",
    "features.remove(TARGET_VAR[0])\n",
    "\n",
    "# Generate the training set. Set random_state to be able to replicate results.\n",
    "# Our train data will contain 80% of train_df.\n",
    "train = train_df.sample(frac=0.8, random_state=1)\n",
    "\n",
    "# Select anything not in the training set (20%) and put it in the validation set.\n",
    "validation = train_df.loc[~train_df.index.isin(train.index)]\n",
    "\n",
    "print \"train (80%):\"\n",
    "print train.shape\n",
    "print \"validation (20%):\"\n",
    "print validation.shape\n",
    "\n",
    "y = train.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 79900  90000 127000 110000 110000 110000 107000 167000 135000  55993\n",
      " 105000 112000 165500 165500 203000 132000 147000 192500 173000 163500\n",
      " 167000 167000 130000 100000 135000 184000 105000 119000  80000 145000\n",
      " 150900 159000  87500 161500 167000 159000 110000 109000  82500 159895\n",
      " 131000 159895 118000 188000 135000 163990 124000 105000 108000 107000\n",
      " 240000 136900 172785 160000 187500  87000 155000 122500  82500 110000\n",
      " 129900 119000  55993 160000 135000  76000 165500  91000  76000 125500\n",
      " 175000 129000 260000  66500  52000 268000 193000  73000 160000  79000\n",
      "  79000 101000 159895 136905 146500  85000 129000 253293 128000  75500\n",
      " 136000 106000 105000 128000 163500 135000 111250  84500 121500 139000\n",
      " 187500 137000 187500 122500 123000 137500  90000  61000 239686 280000\n",
      " 108000  82500 109000 160000 102000  68400 250000  95000 104900 143000\n",
      " 108000 144000  66500 210000 157000 128500 125500  97500 141000  79000\n",
      "  79000 176000 131500 137000  60000 160000 129900 187500 115000 124500\n",
      "  82000 185000 136905 122500 178000 105000  87500  68400 186500 110000\n",
      " 128000  93000 159000 110000 131500 151000 146500  83000 174000 141000\n",
      " 178000 147000 159000 173000  82500 119000 178000 145000 119000  82500\n",
      "  80000 102000 114500 122500 122500 119000 151000 136905 174000 165500\n",
      " 107000 119000 135000 135000 118000 125000 122500 155000 200000 135000\n",
      "  66500 136905 159895  85000 225000  93000  75500 225000  64500 148000\n",
      " 129500 185000  84500 146500 125500 152000 135500 130000 140000  78000\n",
      "  82500 174000  95000  67000  37900  67000  84500 120000 148500 245500\n",
      " 215000 137900 150900 145000 106000 129900 100000  35311 128500  98600\n",
      "  37900  95000 106000 104900 151000 144500 103000 110000 119500 117500\n",
      " 170000 159000 143000 180000 118000 173000 179600 208900 100000 109000\n",
      " 139000 290000 202900  61000  60000 159000 110500 200000 125000  52000\n",
      "  87000 178000  93000 180000 187100 140000 110000 111250 215000 117500\n",
      "  40000 176000  67000 150900 201000 109000  93000 141000 124000 113000\n",
      " 162000 105000 104900 129900 225000 145000 113000 147000 107000 135000\n",
      " 117500 124500]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "0.013698630137\n",
      "Mean Squared Error: 5438123073.23\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=8)\n",
    "neigh.fit(train[features], train[TARGET_VAR[0]])\n",
    "\n",
    "KNN_prediction = (neigh.predict(validation[features]))\n",
    "print KNN_prediction\n",
    "print(neigh.predict_proba(validation[features]))\n",
    "\n",
    "print metrics.accuracy_score(validation.SalePrice, KNN_prediction)\n",
    "                                       \n",
    "print 'Mean Squared Error:', mean_squared_error(KNN_prediction, validation.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Now we're finally ready two use our preprocessed data for training a linear regression model based on the 80 % train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Sqared Error: 507651587.144\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model class.\n",
    "linear_regression_model = LinearRegression() #(normalize=True)?\n",
    "\n",
    "# Fit the model to the 80% training data.\n",
    "linear_regression_model.fit(train[features], train[TARGET_VAR[0]])\n",
    "\n",
    "# Generate our predictions for the validation set.\n",
    "predictions = linear_regression_model.predict(validation[features])\n",
    "\n",
    "predictions = predictions.astype(int)\n",
    "\n",
    "# Compute error between our validation predictions and the actual values.\n",
    "print 'Mean Sqared Error:', mean_squared_error(predictions, validation[TARGET_VAR[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy with the mean squared error (the lower the better)? If yes, we can improve our model by using the entire 100% of our train_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression_model = LinearRegression() #(normalize=True)?\n",
    "linear_regression_model.fit(train_df[features], train_df[TARGET_VAR[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_curve(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Calculate the performance of the model after a set of training data.\"\"\"\n",
    "\n",
    "    # We will vary the training set size so that we have 50 different sizes\n",
    "    sizes = np.round(np.linspace(1, len(X_train), 50))\n",
    "    train_err = np.zeros(len(sizes))\n",
    "    test_err = np.zeros(len(sizes))\n",
    "    \n",
    "    for i, features in enumerate(sizes):\n",
    "        # Create and fit the decision tree regressor model\n",
    "      \n",
    "        linear_regression_model.fit(X_train[:features], y_train[:TARGET_VAR[0]])\n",
    "        \n",
    "        #regressor.fit(X_train[:s], y_train[:s])\n",
    "\n",
    "        # Find the performance on the training and testing set\n",
    "        train_err[i] = performance_metric(y_train[:s], linear_regression_model.predict(X_train[:s]))\n",
    "        test_err[i] = performance_metric(y_test, linear_regression_model.predict(X_test))\n",
    "\n",
    "    # Plot learning curve graph\n",
    "    learning_curve_graph(sizes, train_err, test_err)\n",
    "\n",
    "\n",
    "def learning_curve_graph(sizes, train_err, test_err):\n",
    "    \"\"\"Plot training and test error as a function of the training size.\"\"\"\n",
    "\n",
    "    pl.figure()\n",
    "    pl.title('Decision Trees: Performance vs Training Size')\n",
    "    pl.plot(sizes, test_err, lw=2, label = 'test error')\n",
    "    pl.plot(sizes, train_err, lw=2, label = 'training error')\n",
    "    pl.legend()\n",
    "    pl.xlabel('Training Size')\n",
    "    pl.ylabel('Error')\n",
    "    pl.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'MSSubClass_20', u'MSSubClass_30', u'MSSubClass_40', u'MSSubClass_45',\n",
      "       u'MSSubClass_50', u'MSSubClass_60', u'MSSubClass_70', u'MSSubClass_75',\n",
      "       u'MSSubClass_80', u'MSSubClass_85',\n",
      "       ...\n",
      "       u'GarageArea', u'WoodDeckSF', u'OpenPorchSF', u'EnclosedPorch',\n",
      "       u'3SsnPorch', u'ScreenPorch', u'PoolArea', u'MiscVal', u'YrSold',\n",
      "       u'SalePrice'],\n",
      "      dtype='object', length=314)\n",
      "Index([u'MSSubClass_20', u'MSSubClass_30', u'MSSubClass_40', u'MSSubClass_45',\n",
      "       u'MSSubClass_50', u'MSSubClass_60', u'MSSubClass_70', u'MSSubClass_75',\n",
      "       u'MSSubClass_80', u'MSSubClass_85',\n",
      "       ...\n",
      "       u'GarageArea', u'WoodDeckSF', u'OpenPorchSF', u'EnclosedPorch',\n",
      "       u'3SsnPorch', u'ScreenPorch', u'PoolArea', u'MiscVal', u'YrSold',\n",
      "       u'SalePrice'],\n",
      "      dtype='object', length=314)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'features' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-500-8e111e130dd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlearning_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTARGET_VAR\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTARGET_VAR\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#learning_curve(X_train, y_train, X_test, y_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-491-24fb252b4168>\u001b[0m in \u001b[0;36mlearning_curve\u001b[1;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtest_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'features' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#print train.columns\n",
    "#print validation.columns\n",
    "learning_curve(train[TARGET_VAR[0]], train[features], validation[TARGET_VAR[0]], validation[features])\n",
    "#learning_curve(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this improved model to predict the housing prices of our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459L,)"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = linear_regression_model.predict(test_df[features])\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to a lack of a validation set, we are now no longer able to compute the mean squared error. But by uploading our predictions for the test set to Kaggle, we can get an even better sense of how good we're doing. This requires preparing a csv file according to Kaggle's requirements: one column with the ID, and one column with the predicted SalePrice. We create a function we can also reuse for other models than Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def export_csv(predictions, model):\n",
    "    # Get the IDs from the indeces we earlier stored in test_df.\n",
    "    ids = test_df.index.values\n",
    "\n",
    "    # Stack the IDs and predictions into a numpy array and transpose it into\n",
    "    # vertical form.\n",
    "    submission = np.vstack((ids, predictions)).T\n",
    "\n",
    "    # Convert the submission array into a Pandas DataFrame object.\n",
    "    submission = pd.DataFrame(data=submission, columns=['Id', 'SalePrice'])\n",
    "\n",
    "    # Convert Id from float to int to avoid .0 notation.\n",
    "    submission['Id'] = submission['Id'].astype(int)\n",
    "    \n",
    "    # Convert possible negative predicted values to 0\n",
    "    submission[submission < 0] = 0\n",
    "\n",
    "    # Print the shape of the newly created submission DataFrame.\n",
    "    print 'Submission:', submission.shape\n",
    "    \n",
    "    # Export the submissions to a csv file.\n",
    "    submission.to_csv('submission_' + model + '.csv', sept=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the export_csv function to print our Linear Regression predictions to a csv file called submission_linear_regression.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission: (1459, 2)\n"
     ]
    }
   ],
   "source": [
    "export_csv(predictions, 'linear_regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e-02   5.06040470e+08]\n",
      " [  2.00000000e-02   5.04563056e+08]\n",
      " [  4.00000000e-02   5.02574653e+08]\n",
      " [  8.00000000e-02   5.01932499e+08]\n",
      " [  1.60000000e-01   5.06965627e+08]\n",
      " [  3.20000000e-01   5.21892592e+08]\n",
      " [  6.40000000e-01   5.45145694e+08]\n",
      " [  1.28000000e+00   5.68197705e+08]\n",
      " [  2.56000000e+00   5.80828262e+08]\n",
      " [  5.12000000e+00   5.77520320e+08]\n",
      " [  1.02400000e+01   5.61227283e+08]\n",
      " [  2.04800000e+01   5.41507846e+08]\n",
      " [  4.09600000e+01   5.29925289e+08]]\n",
      "Submission: (1459, 2)\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24, 20.48, 40.96]\n",
    "\n",
    "mean_squared_errors = np.zeros((len(alphas), 2))\n",
    "\n",
    "for i, alph in enumerate(alphas):\n",
    "    ridge_regression_model = Ridge(alpha = alph)\n",
    "    ridge_regression_model.fit(train[features], train[TARGET_VAR[0]])\n",
    "    predictions = ridge_regression_model.predict(validation[features])\n",
    "    \n",
    "    mean_squared_errors[i] = [alph, mean_squared_error(predictions, validation[TARGET_VAR[0]])]\n",
    "\n",
    "print mean_squared_errors\n",
    "\n",
    "ridge_regression_model = Ridge(alpha = 0.08)\n",
    "ridge_regression_model.fit(train_df[features], train_df[TARGET_VAR[0]])\n",
    "predictions = ridge_regression_model.predict(test_df[features])\n",
    "\n",
    "export_csv(predictions, 'ridge_regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01     1.063450e-08\n",
      "0.02     6.292937e-09\n",
      "0.04     5.962644e-09\n",
      "0.08     9.068000e-09\n",
      "0.16     1.672000e-08\n",
      "0.32     3.001694e-08\n",
      "0.64     5.323160e-08\n",
      "1.28     9.441491e-08\n",
      "2.56     1.683288e-07\n",
      "5.12     3.023296e-07\n",
      "10.24    5.442833e-07\n",
      "20.48    9.816139e-07\n",
      "40.96    1.778659e-06\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0xd365208>"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def rmse_cv(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "    return(rmse)\n",
    "\n",
    "alphas = [0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24, 20.48, 40.96]\n",
    "#alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\n",
    "cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n",
    "            for alpha in alphas]\n",
    "\n",
    "cv_ridge = pd.Series(cv_ridge, index = alphas)\n",
    "\n",
    "print cv_ridge\n",
    "\n",
    "cv_ridge.plot(title = \"Validation - Just Do It\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.9626436138496743e-09"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_ridge.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor\n",
    "Now we can try out another method: Random Forest Regressor. It works pretty much the same like the Linear Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Sqared Error: 639153795.737\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model.\n",
    "# Note: The parameters given here are not really optimized yet. Apparantly, n_estimator\n",
    "#       is the one we should pay the most attention to. For details see:\n",
    "#       http://scikit-learn.org/stable/modules/ensemble.html#parameters\n",
    "random_forest_model = RandomForestRegressor(n_estimators=100, min_samples_leaf=10, random_state=1)\n",
    "\n",
    "# Fit the model to the 80% train data.\n",
    "random_forest_model.fit(train[features], train[TARGET_VAR[0]])\n",
    "\n",
    "# Make predictions for our validation set.\n",
    "predictions = random_forest_model.predict(validation[features])\n",
    "\n",
    "# Compute the error.\n",
    "print 'Mean Sqared Error:', mean_squared_error(predictions, validation[TARGET_VAR[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, when we are happy with the Mean Squared Error, we can fit the model to our 100% train_df dataset, predict the values for Kaggle's test dataset, and export the results into a file called submission_random_forest_regressor.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TO DO: Optimize parameters (see comments in the codeblock above).\n",
    "random_forest_model = RandomForestRegressor(n_estimators=100, min_samples_leaf=10, random_state=1)\n",
    "random_forest_model.fit(train_df[features], train_df[TARGET_VAR[0]])\n",
    "\n",
    "predictions = random_forest_model.predict(test_df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission: (1459, 2)\n"
     ]
    }
   ],
   "source": [
    "export_csv(predictions, 'random_forest_regressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
